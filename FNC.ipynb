{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryguy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset csv (C:/Users/ryguy/.cache/huggingface/datasets/nid989___csv/nid989--FNC-1-ba6c09ed0c9efe30/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
      "100%|██████████| 3/3 [00:00<00:00, 12.92it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"nid989/FNC-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "test = dataset['test']\n",
    "val = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Headline': 'Kimye’s Fault? Anna Wintour’s NYC ‘Vogue’ Offices Hit With Disgusting Rat Infestation: ‘Rat Droppings On Desks’',\n",
       " 'articleBody': 'YouTube prankster Josh Paler Lin decided give man named Thomas money follow see spends it. His first stop local liquor store instead buying alcohol purchased food gave away others need. A homeless man given chance, touched awful lot surprised people. A viral online video YouTube prankster Josh Paler Lin tells story homeless man named Thomas desire let circumstances keep selfless - using newfound money help himself, others. Josh made video, received 11 million hits Wednesday morning, idea give random homeless person $100 secretly follow see spends it. He meets elderly man gray hair, mustache dark jacket named Thomas holding sign along highway. He tells Josh he\\'s trying get enough buy something eat Josh shocks $100. \"Oh goodness. Oh way,\" says Josh pulls five $20 bills wallet. \"Oh brother, sure? I\\'m starting tear here. That\\'s like incredible.\" Josh reassures totally fine hug. Thomas says never something like happen life. The next part video seemed expected many viewers Thomas surprised all. He packed stuff began walking road liquor store. The video records exit store several bags, buy alcohol. He purchased food delivered needy people nearby park. Josh appears tell Thomas going on. \"I feel like I owe apology,\" Josh said reaches grab Thomas\\' hand. \"You went liquor store earlier —\" \"You thought I going get smacked drunk, huh?\" Thomas interrupts. \"But there\\'s things money can\\'t buy I get happiness I\\'m doing.\" Ironically Josh\\'s confession man sitting picnic table behind takes swig bottle wrapped plastic bag. But Josh wanted contribute Thomas\\'s good deeds. He reached wallet gave another $100. \"You touched heart,\" told new friend. Thomas stunned again. \"I know say. Usually I\\'m pretty talkative,\" replied. Thomas explained recently quit job caring ailing stepfather ultimately died cancer. His mom died two weeks later kidney failure suddenly building sold leaving Thomas without place live. \"There\\'s lot people victims circumstance. There\\'s lot good people homeless,\" said. \"You\\'re good … bump good people. That\\'s it. You\\'re different path somehow life. I can\\'t explain it.\" But Josh trying change path. He established fund-raising campaign help Thomas get new home job. The fund goal $10,000 surpassed $55,000 Wednesday morning. \"People think I changed life,\" Josh told TODAY. \"For me, completely opposite. I feel changed life.\" USING A MOBILE DEVICE? CLICK HERE TO SEE THE VIDEO. jlandau@nydailynews.com',\n",
       " 'Stance': 3}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "import re\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove puncuation \n",
    "# NOTE: HANDLE Sentence Level\n",
    "import re\n",
    "import numpy as np\n",
    "def cleanText(textData):\n",
    "    # Lowercase\n",
    "    textDataLower = np.array(list(map(lambda x: x.lower(),textData)))\n",
    "    # Expand contradictions\n",
    "    textNoContras = np.array(list(map(lambda x: decontracted(x),textDataLower)))\n",
    "    # Remove Puncuation\n",
    "    textDataNoPunc = np.array(list(map(lambda x: re.sub(r'\\.|,|\\(|\\)|\\?|!|\\n|;|:','',x), textNoContras)))\n",
    "    # Remove numbers\n",
    "    textNoNumbers = np.array(list(map(lambda x: re.sub(r'(0|1|2|3|4|5|6|7|8|9|0)+','',x),textDataNoPunc)))\n",
    "    # Remove extra spaces\n",
    "    cleanedText = np.array(list(map(lambda x: re.sub(r' +',' ',x),textNoNumbers)))\n",
    "    return np.array(list(map(lambda x: x.split(' '),cleanedText)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryguy\\AppData\\Local\\Temp\\ipykernel_4496\\3918129584.py:16: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(list(map(lambda x: x.split(' '),cleanedText)))\n"
     ]
    }
   ],
   "source": [
    "trainHeadlines = cleanText(train['Headline'])\n",
    "trainBodies = cleanText(train['articleBody'])\n",
    "valHeadlines = cleanText(val['Headline'])\n",
    "valBodies = cleanText(val['articleBody'])\n",
    "testHeadlines = cleanText(test['Headline'])\n",
    "testBodies = cleanText(test['articleBody'])\n",
    "yTrain = train['Stance']\n",
    "yVal = val['Stance']\n",
    "yTest = test['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 199.5/199.5MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorSize = 50\n",
    "def vectorizeWord(word):\n",
    "    if (word in wv): return wv[word][0:vectorSize]\n",
    "    return [0]*vectorSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeSent(sent):\n",
    "    return np.array(list(map(lambda x: vectorizeWord(x),sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryguy\\AppData\\Local\\Temp\\ipykernel_4496\\3930145267.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vectorTrainHeadlines = np.array(list(map(lambda x: vectorizeSent(x),trainHeadlines)))\n",
      "C:\\Users\\ryguy\\AppData\\Local\\Temp\\ipykernel_4496\\3930145267.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vectorTrainBodies = np.array(list(map(lambda x: vectorizeSent(x),trainBodies)))\n",
      "C:\\Users\\ryguy\\AppData\\Local\\Temp\\ipykernel_4496\\3930145267.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vectorValHeadlines = np.array(list(map(lambda x: vectorizeSent(x),valHeadlines)))\n",
      "C:\\Users\\ryguy\\AppData\\Local\\Temp\\ipykernel_4496\\3930145267.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vectorValBodies = np.array(list(map(lambda x: vectorizeSent(x),valBodies)))\n",
      "C:\\Users\\ryguy\\AppData\\Local\\Temp\\ipykernel_4496\\3930145267.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vectorTestHeadlines = np.array(list(map(lambda x: vectorizeSent(x),testHeadlines)))\n",
      "C:\\Users\\ryguy\\AppData\\Local\\Temp\\ipykernel_4496\\3930145267.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vectorTestBodies = np.array(list(map(lambda x: vectorizeSent(x),testBodies)))\n"
     ]
    }
   ],
   "source": [
    "vectorTrainHeadlines = np.array(list(map(lambda x: vectorizeSent(x),trainHeadlines)))\n",
    "vectorTrainBodies = np.array(list(map(lambda x: vectorizeSent(x),trainBodies)))\n",
    "\n",
    "vectorValHeadlines = np.array(list(map(lambda x: vectorizeSent(x),valHeadlines)))\n",
    "vectorValBodies = np.array(list(map(lambda x: vectorizeSent(x),valBodies)))\n",
    "\n",
    "vectorTestHeadlines = np.array(list(map(lambda x: vectorizeSent(x),testHeadlines)))\n",
    "vectorTestBodies = np.array(list(map(lambda x: vectorizeSent(x),testBodies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Average Vector of all entries\n",
    "def getAverageVector(vectors):\n",
    "    return np.average(vectors,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "averageVectorTrainHeadlines = np.array(list(map(lambda x: getAverageVector(x),vectorTrainHeadlines)))\n",
    "averageVectorBodyTrain = np.array(list(map(lambda x: getAverageVector(x),vectorTrainBodies)))\n",
    "\n",
    "averageVectorValHeadlines = np.array(list(map(lambda x: getAverageVector(x),vectorValHeadlines)))\n",
    "averageVectorValBodies = np.array(list(map(lambda x: getAverageVector(x),vectorValBodies)))\n",
    "\n",
    "averageVectorTestHeadlines = np.array(list(map(lambda x: getAverageVector(x),vectorTestHeadlines)))\n",
    "averageVectorTestBodies = np.array(list(map(lambda x: getAverageVector(x),vectorTestBodies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headlines = 10\n",
    "# Body = 90\n",
    "def pad(arr,size):\n",
    "    if (len(arr)>size): return arr[0:size]\n",
    "    elif len(arr)<size: return np.append(arr, np.array((size-len(arr))*[[0]*50]),axis=0)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "padTrainHeadlines = np.array(list(map(lambda x: pad(x,10),vectorTrainHeadlines)))\n",
    "padValHeadlines = np.array(list(map(lambda x: pad(x,10),vectorValHeadlines)))\n",
    "padTestHeadlines = np.array(list(map(lambda x: pad(x,10),vectorTestHeadlines)))\n",
    "\n",
    "padTrainBodies = np.array(list(map(lambda x: pad(x,90),vectorTrainBodies)))\n",
    "padValBodies = np.array(list(map(lambda x: pad(x,90),vectorValBodies)))\n",
    "padTestBodies = np.array(list(map(lambda x: pad(x,90),vectorTestBodies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatTrain = []\n",
    "for i in range(len(vectorTrainHeadlines)):\n",
    "    concatTrain.append(np.concatenate((padTrainHeadlines[i],[averageVectorTrainHeadlines[i]+averageVectorBodyTrain[i]],padTrainBodies[i])))\n",
    "concatTrain = np.array(concatTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40476, 101, 50)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(40476, 101, 50)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatVal = []\n",
    "for i in range(len(vectorValHeadlines)):\n",
    "    concatVal.append(np.concatenate((padValHeadlines[i],[averageVectorValHeadlines[i]+averageVectorValBodies[i]],padValBodies[i])))\n",
    "concatVal = np.array(concatVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4498, 101, 50)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatVal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatTest = []\n",
    "for i in range(len(vectorTestHeadlines)):\n",
    "    concatTest.append(np.concatenate((padTestHeadlines[i],[averageVectorTestHeadlines[i]+averageVectorTestBodies[i]],padTestBodies[i])))\n",
    "concatTest = np.array(concatTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4998, 101, 50)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelsToBinary(labels):\n",
    "    return np.array(list(map(lambda x: 0 if x==3 else 1,labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrainBinary = labelsToBinary(yTrain)\n",
    "yValBinary = labelsToBinary(yVal)\n",
    "yTestBinary = labelsToBinary(yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29544, 10932, 0, 0]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = [0,0,0,0]\n",
    "for i in yTrainBinary:\n",
    "    counts[i]+=1\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(101,50),dtype=tf.float32))\n",
    "\n",
    "model.add(layers.Conv1D(32,3))\n",
    "model.add(layers.MaxPooling1D())\n",
    "\n",
    "model.add(layers.Conv1D(48,3))\n",
    "model.add(layers.MaxPooling1D())\n",
    "\n",
    "model.add(layers.Bidirectional(layers.LSTM(64)))\n",
    "model.add(layers.Dropout(.2))\n",
    "\n",
    "model.add(layers.Dense(256))\n",
    "model.add(layers.Dense(256))\n",
    "model.add(layers.Dense(2,activation=\"softmax\"))\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1265/1265 [==============================] - 45s 29ms/step - loss: 0.4404 - accuracy: 0.7883 - val_loss: 0.2940 - val_accuracy: 0.8626\n",
      "Epoch 2/15\n",
      "1265/1265 [==============================] - 35s 27ms/step - loss: 0.2531 - accuracy: 0.8887 - val_loss: 0.2054 - val_accuracy: 0.9117\n",
      "Epoch 3/15\n",
      "1265/1265 [==============================] - 32s 25ms/step - loss: 0.1794 - accuracy: 0.9234 - val_loss: 0.1730 - val_accuracy: 0.9291\n",
      "Epoch 4/15\n",
      "1265/1265 [==============================] - 32s 25ms/step - loss: 0.1423 - accuracy: 0.9417 - val_loss: 0.1609 - val_accuracy: 0.9293\n",
      "Epoch 5/15\n",
      "1265/1265 [==============================] - 30s 24ms/step - loss: 0.1147 - accuracy: 0.9533 - val_loss: 0.1515 - val_accuracy: 0.9422\n",
      "Epoch 6/15\n",
      "1265/1265 [==============================] - 31s 24ms/step - loss: 0.1002 - accuracy: 0.9600 - val_loss: 0.1354 - val_accuracy: 0.9453\n",
      "Epoch 7/15\n",
      "1265/1265 [==============================] - 30s 24ms/step - loss: 0.0837 - accuracy: 0.9676 - val_loss: 0.1272 - val_accuracy: 0.9482\n",
      "Epoch 8/15\n",
      "1265/1265 [==============================] - 32s 25ms/step - loss: 0.0766 - accuracy: 0.9701 - val_loss: 0.1212 - val_accuracy: 0.9529\n",
      "Epoch 9/15\n",
      "1265/1265 [==============================] - 31s 24ms/step - loss: 0.0694 - accuracy: 0.9734 - val_loss: 0.1246 - val_accuracy: 0.9526\n",
      "Epoch 10/15\n",
      "1265/1265 [==============================] - 30s 24ms/step - loss: 0.0638 - accuracy: 0.9757 - val_loss: 0.1207 - val_accuracy: 0.9535\n",
      "Epoch 11/15\n",
      "1265/1265 [==============================] - 30s 24ms/step - loss: 0.0580 - accuracy: 0.9784 - val_loss: 0.1478 - val_accuracy: 0.9549\n",
      "Epoch 12/15\n",
      "1265/1265 [==============================] - 30s 24ms/step - loss: 0.0555 - accuracy: 0.9777 - val_loss: 0.1237 - val_accuracy: 0.9591\n",
      "Epoch 13/15\n",
      "1265/1265 [==============================] - 30s 23ms/step - loss: 0.0497 - accuracy: 0.9812 - val_loss: 0.1145 - val_accuracy: 0.9580\n",
      "Epoch 14/15\n",
      "1265/1265 [==============================] - 29s 23ms/step - loss: 0.0466 - accuracy: 0.9825 - val_loss: 0.1053 - val_accuracy: 0.9604\n",
      "Epoch 15/15\n",
      "1265/1265 [==============================] - 31s 25ms/step - loss: 0.0459 - accuracy: 0.9827 - val_loss: 0.1224 - val_accuracy: 0.9591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1fd5c61a590>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(list(concatTrain)),np.array(list(yTrainBinary)),\n",
    "          validation_data=(np.array(list(concatVal)),np.array(list(yValBinary))),epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 3s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(concatTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predsActual = np.array(list(map(lambda x: np.argmax(x),preds)))\n",
    "predsActual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9635854341736695"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(yTestBinary,predsActual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data for training the second order model\n",
    "def getSecondOrderData(x,y):\n",
    "    newX = []\n",
    "    newY = []\n",
    "    for i in range(len(y)):\n",
    "        if (y[i]!=3):\n",
    "            newX.append(x[i])\n",
    "            newY.append(y[i])\n",
    "    return (np.array(newX),np.array(newY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainTwo, yTrainTwo = getSecondOrderData(concatTrain,yTrain)\n",
    "xValTwo, yValTwo = getSecondOrderData(concatVal, yVal)\n",
    "xTestTwo, yTestTwo = getSecondOrderData(concatTest, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTwo = tf.keras.Sequential()\n",
    "modelTwo.add(tf.keras.Input(shape=(101,50),dtype=tf.float32))\n",
    "\n",
    "modelTwo.add(layers.Conv1D(32,3))\n",
    "modelTwo.add(layers.MaxPooling1D())\n",
    "\n",
    "modelTwo.add(layers.Conv1D(48,3))\n",
    "modelTwo.add(layers.MaxPooling1D())\n",
    "\n",
    "modelTwo.add(layers.Bidirectional(layers.LSTM(64)))\n",
    "modelTwo.add(layers.Dropout(.2))\n",
    "\n",
    "modelTwo.add(layers.Dense(256))\n",
    "modelTwo.add(layers.Dense(256))\n",
    "modelTwo.add(layers.Dense(3,activation=\"softmax\"))\n",
    "modelTwo.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n",
      "342/342 [==============================] - 14s 23ms/step - loss: 0.6934 - accuracy: 0.7055 - val_loss: 0.6322 - val_accuracy: 0.7166\n",
      "Epoch 2/13\n",
      "342/342 [==============================] - 7s 20ms/step - loss: 0.5032 - accuracy: 0.7927 - val_loss: 0.4522 - val_accuracy: 0.8119\n",
      "Epoch 3/13\n",
      "342/342 [==============================] - 7s 21ms/step - loss: 0.3577 - accuracy: 0.8618 - val_loss: 0.3761 - val_accuracy: 0.8512\n",
      "Epoch 4/13\n",
      "342/342 [==============================] - 7s 21ms/step - loss: 0.2735 - accuracy: 0.8925 - val_loss: 0.2979 - val_accuracy: 0.8888\n",
      "Epoch 5/13\n",
      "342/342 [==============================] - 7s 21ms/step - loss: 0.2083 - accuracy: 0.9188 - val_loss: 0.2708 - val_accuracy: 0.8997\n",
      "Epoch 6/13\n",
      "342/342 [==============================] - 7s 21ms/step - loss: 0.1760 - accuracy: 0.9333 - val_loss: 0.2758 - val_accuracy: 0.9038\n",
      "Epoch 7/13\n",
      "342/342 [==============================] - 6s 19ms/step - loss: 0.1463 - accuracy: 0.9417 - val_loss: 0.3088 - val_accuracy: 0.9047\n",
      "Epoch 8/13\n",
      "342/342 [==============================] - 6s 18ms/step - loss: 0.1270 - accuracy: 0.9505 - val_loss: 0.2626 - val_accuracy: 0.9156\n",
      "Epoch 9/13\n",
      "342/342 [==============================] - 7s 19ms/step - loss: 0.1075 - accuracy: 0.9608 - val_loss: 0.2265 - val_accuracy: 0.9256\n",
      "Epoch 10/13\n",
      "342/342 [==============================] - 6s 19ms/step - loss: 0.0861 - accuracy: 0.9694 - val_loss: 0.2669 - val_accuracy: 0.9214\n",
      "Epoch 11/13\n",
      "342/342 [==============================] - 6s 19ms/step - loss: 0.0798 - accuracy: 0.9697 - val_loss: 0.2475 - val_accuracy: 0.9273\n",
      "Epoch 12/13\n",
      "342/342 [==============================] - 7s 20ms/step - loss: 0.0662 - accuracy: 0.9770 - val_loss: 0.2702 - val_accuracy: 0.9273\n",
      "Epoch 13/13\n",
      "342/342 [==============================] - 7s 19ms/step - loss: 0.0557 - accuracy: 0.9815 - val_loss: 0.2818 - val_accuracy: 0.9339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1fd46db5a20>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelTwo.fit(np.array(list(xTrainTwo)),np.array(list(yTrainTwo)),\n",
    "          validation_data=(np.array(list(xValTwo)),np.array(list(yValTwo))),epochs=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 2s 10ms/step\n",
      "157/157 [==============================] - 2s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "# Get First order predictions\n",
    "finalPreds = []\n",
    "orderOnePreds = np.array(list(map(lambda x: np.argmax(x),model.predict(concatTest))))\n",
    "orderTwoPreds = np.array(list(map(lambda x: np.argmax(x),modelTwo.predict(concatTest))))\n",
    "for i in range(len(orderOnePreds)):\n",
    "    if (orderOnePreds[i]==0):\n",
    "        finalPreds.append(3)\n",
    "    else:\n",
    "        finalPreds.append(orderTwoPreds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[362, 57, 928, 3651]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = [0,0,0,0]\n",
    "for i in finalPreds:\n",
    "    counts[i]+=1\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.951780712284914"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(finalPreds,yTest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
