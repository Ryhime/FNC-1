{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryguy\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset csv (C:/Users/ryguy/.cache/huggingface/datasets/nid989___csv/nid989--FNC-1-ba6c09ed0c9efe30/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d)\n",
      "100%|██████████| 3/3 [00:00<00:00, 10.16it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"nid989/FNC-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "test = dataset['test']\n",
    "val = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Headline': 'Kimye’s Fault? Anna Wintour’s NYC ‘Vogue’ Offices Hit With Disgusting Rat Infestation: ‘Rat Droppings On Desks’',\n",
       " 'articleBody': 'YouTube prankster Josh Paler Lin decided give man named Thomas money follow see spends it. His first stop local liquor store instead buying alcohol purchased food gave away others need. A homeless man given chance, touched awful lot surprised people. A viral online video YouTube prankster Josh Paler Lin tells story homeless man named Thomas desire let circumstances keep selfless - using newfound money help himself, others. Josh made video, received 11 million hits Wednesday morning, idea give random homeless person $100 secretly follow see spends it. He meets elderly man gray hair, mustache dark jacket named Thomas holding sign along highway. He tells Josh he\\'s trying get enough buy something eat Josh shocks $100. \"Oh goodness. Oh way,\" says Josh pulls five $20 bills wallet. \"Oh brother, sure? I\\'m starting tear here. That\\'s like incredible.\" Josh reassures totally fine hug. Thomas says never something like happen life. The next part video seemed expected many viewers Thomas surprised all. He packed stuff began walking road liquor store. The video records exit store several bags, buy alcohol. He purchased food delivered needy people nearby park. Josh appears tell Thomas going on. \"I feel like I owe apology,\" Josh said reaches grab Thomas\\' hand. \"You went liquor store earlier —\" \"You thought I going get smacked drunk, huh?\" Thomas interrupts. \"But there\\'s things money can\\'t buy I get happiness I\\'m doing.\" Ironically Josh\\'s confession man sitting picnic table behind takes swig bottle wrapped plastic bag. But Josh wanted contribute Thomas\\'s good deeds. He reached wallet gave another $100. \"You touched heart,\" told new friend. Thomas stunned again. \"I know say. Usually I\\'m pretty talkative,\" replied. Thomas explained recently quit job caring ailing stepfather ultimately died cancer. His mom died two weeks later kidney failure suddenly building sold leaving Thomas without place live. \"There\\'s lot people victims circumstance. There\\'s lot good people homeless,\" said. \"You\\'re good … bump good people. That\\'s it. You\\'re different path somehow life. I can\\'t explain it.\" But Josh trying change path. He established fund-raising campaign help Thomas get new home job. The fund goal $10,000 surpassed $55,000 Wednesday morning. \"People think I changed life,\" Josh told TODAY. \"For me, completely opposite. I feel changed life.\" USING A MOBILE DEVICE? CLICK HERE TO SEE THE VIDEO. jlandau@nydailynews.com',\n",
       " 'Stance': 3}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "import re\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ryguy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove puncuation \n",
    "# NOTE: HANDLE Sentence Level\n",
    "import re\n",
    "import numpy as np\n",
    "def cleanText(textData):\n",
    "    # Lowercase\n",
    "    textDataLower = np.array(list(map(lambda x: x.lower(),textData)))\n",
    "    # Expand contradictions\n",
    "    textNoContras = np.array(list(map(lambda x: decontracted(x),textDataLower)))\n",
    "    # Remove Puncuation\n",
    "    textDataNoPunc = np.array(list(map(lambda x: re.sub(r'\\.|,|\\(|\\)|\\?|!|\\n|;|:','',x), textNoContras)))\n",
    "    # Remove numbers\n",
    "    textNoNumbers = np.array(list(map(lambda x: re.sub(r'(0|1|2|3|4|5|6|7|8|9|0)+','',x),textDataNoPunc)))\n",
    "    # Remove extra spaces\n",
    "    cleanedText = np.array(list(map(lambda x: re.sub(r' +',' ',x),textNoNumbers)))\n",
    "    # Split\n",
    "    splitText = np.array(list(map(lambda x: x.split(' '),cleanedText)))\n",
    "    # Remove stopwords\n",
    "    english_stopwords = stopwords.words('english')\n",
    "    return np.array(list(map(lambda x: np.array([t for t in x if t not in english_stopwords]), splitText)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryguy\\AppData\\Local\\Temp\\ipykernel_10928\\1580574302.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  splitText = np.array(list(map(lambda x: x.split(' '),cleanedText)))\n",
      "C:\\Users\\ryguy\\AppData\\Local\\Temp\\ipykernel_10928\\1580574302.py:20: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(list(map(lambda x: np.array([t for t in x if t not in english_stopwords]), splitText)))\n"
     ]
    }
   ],
   "source": [
    "trainHeadlines = cleanText(train['Headline'])\n",
    "trainBodies = cleanText(train['articleBody'])\n",
    "valHeadlines = cleanText(val['Headline'])\n",
    "valBodies = cleanText(val['articleBody'])\n",
    "testHeadlines = cleanText(test['Headline'])\n",
    "testBodies = cleanText(test['articleBody'])\n",
    "yTrain = train['Stance']\n",
    "yVal = val['Stance']\n",
    "yTest = test['Stance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('glove-twitter-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorSize = 50\n",
    "def vectorizeWord(word):\n",
    "    if (word in wv): return wv[word][0:vectorSize]\n",
    "    return [0]*vectorSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeSent(sent):\n",
    "    return np.array(list(map(lambda x: vectorizeWord(x),sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryguy\\AppData\\Local\\Temp\\ipykernel_10928\\3930145267.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vectorTrainHeadlines = np.array(list(map(lambda x: vectorizeSent(x),trainHeadlines)))\n",
      "C:\\Users\\ryguy\\AppData\\Local\\Temp\\ipykernel_10928\\3930145267.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vectorTrainBodies = np.array(list(map(lambda x: vectorizeSent(x),trainBodies)))\n",
      "C:\\Users\\ryguy\\AppData\\Local\\Temp\\ipykernel_10928\\3930145267.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vectorValHeadlines = np.array(list(map(lambda x: vectorizeSent(x),valHeadlines)))\n",
      "C:\\Users\\ryguy\\AppData\\Local\\Temp\\ipykernel_10928\\3930145267.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vectorValBodies = np.array(list(map(lambda x: vectorizeSent(x),valBodies)))\n",
      "C:\\Users\\ryguy\\AppData\\Local\\Temp\\ipykernel_10928\\3930145267.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vectorTestHeadlines = np.array(list(map(lambda x: vectorizeSent(x),testHeadlines)))\n",
      "C:\\Users\\ryguy\\AppData\\Local\\Temp\\ipykernel_10928\\3930145267.py:8: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  vectorTestBodies = np.array(list(map(lambda x: vectorizeSent(x),testBodies)))\n"
     ]
    }
   ],
   "source": [
    "vectorTrainHeadlines = np.array(list(map(lambda x: vectorizeSent(x),trainHeadlines)))\n",
    "vectorTrainBodies = np.array(list(map(lambda x: vectorizeSent(x),trainBodies)))\n",
    "\n",
    "vectorValHeadlines = np.array(list(map(lambda x: vectorizeSent(x),valHeadlines)))\n",
    "vectorValBodies = np.array(list(map(lambda x: vectorizeSent(x),valBodies)))\n",
    "\n",
    "vectorTestHeadlines = np.array(list(map(lambda x: vectorizeSent(x),testHeadlines)))\n",
    "vectorTestBodies = np.array(list(map(lambda x: vectorizeSent(x),testBodies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Average Vector of all entries\n",
    "def getAverageVector(vectors):\n",
    "    return np.average(vectors,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "averageVectorTrainHeadlines = np.array(list(map(lambda x: getAverageVector(x),vectorTrainHeadlines)))\n",
    "averageVectorBodyTrain = np.array(list(map(lambda x: getAverageVector(x),vectorTrainBodies)))\n",
    "\n",
    "averageVectorValHeadlines = np.array(list(map(lambda x: getAverageVector(x),vectorValHeadlines)))\n",
    "averageVectorValBodies = np.array(list(map(lambda x: getAverageVector(x),vectorValBodies)))\n",
    "\n",
    "averageVectorTestHeadlines = np.array(list(map(lambda x: getAverageVector(x),vectorTestHeadlines)))\n",
    "averageVectorTestBodies = np.array(list(map(lambda x: getAverageVector(x),vectorTestBodies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headlines = 10\n",
    "# Body = 90\n",
    "def pad(arr,size):\n",
    "    if (len(arr)>size): return arr[0:size]\n",
    "    elif len(arr)<size: return np.append(arr, np.array((size-len(arr))*[[0]*50]),axis=0)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "padTrainHeadlines = np.array(list(map(lambda x: pad(x,10),vectorTrainHeadlines)))\n",
    "padValHeadlines = np.array(list(map(lambda x: pad(x,10),vectorValHeadlines)))\n",
    "padTestHeadlines = np.array(list(map(lambda x: pad(x,10),vectorTestHeadlines)))\n",
    "\n",
    "padTrainBodies = np.array(list(map(lambda x: pad(x,90),vectorTrainBodies)))\n",
    "padValBodies = np.array(list(map(lambda x: pad(x,90),vectorValBodies)))\n",
    "padTestBodies = np.array(list(map(lambda x: pad(x,90),vectorTestBodies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatTrain = []\n",
    "for i in range(len(vectorTrainHeadlines)):\n",
    "    concatTrain.append(np.concatenate((padTrainHeadlines[i],[averageVectorTrainHeadlines[i]+averageVectorBodyTrain[i]],padTrainBodies[i])))\n",
    "concatTrain = np.array(concatTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40476, 101, 50)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatVal = []\n",
    "for i in range(len(vectorValHeadlines)):\n",
    "    concatVal.append(np.concatenate((padValHeadlines[i],[averageVectorValHeadlines[i]+averageVectorValBodies[i]],padValBodies[i])))\n",
    "concatVal = np.array(concatVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4498, 101, 50)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatVal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatTest = []\n",
    "for i in range(len(vectorTestHeadlines)):\n",
    "    concatTest.append(np.concatenate((padTestHeadlines[i],[averageVectorTestHeadlines[i]+averageVectorTestBodies[i]],padTestBodies[i])))\n",
    "concatTest = np.array(concatTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4998, 101, 50)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelsToBinary(labels):\n",
    "    return np.array(list(map(lambda x: 0 if x==3 else 1,labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrainBinary = labelsToBinary(yTrain)\n",
    "yValBinary = labelsToBinary(yVal)\n",
    "yTestBinary = labelsToBinary(yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29544, 10932, 0, 0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = [0,0,0,0]\n",
    "for i in yTrainBinary:\n",
    "    counts[i]+=1\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "# https://www.kaggle.com/code/haithemhermessi/attention-mechanism-keras-as-simple-as-possible\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    "    def build(self,input_shape):\n",
    "        self.w=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")\n",
    "        super(attention,self).build(input_shape)\n",
    "    def call(self,x):\n",
    "        et=K.squeeze(K.tanh(K.dot(x,self.w)+self.b), axis=-1)\n",
    "        at=K.softmax(et)\n",
    "        at=K.expand_dims(at,axis=-1)\n",
    "        output=x*at\n",
    "        return K.sum(output,axis=1)\n",
    "    \n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[-1])\n",
    "    def get_config(self):\n",
    "        return super(attention,self).get_config()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(101,50),dtype=tf.float32))\n",
    "\n",
    "model.add(layers.Bidirectional(layers.LSTM(128,return_sequences=True)))\n",
    "\n",
    "model.add(layers.Conv1D(64,3))\n",
    "model.add(layers.MaxPooling1D())\n",
    "\n",
    "model.add(layers.Bidirectional(layers.LSTM(128,return_sequences=True)))\n",
    "\n",
    "model.add(layers.Dense(256))\n",
    "model.add(layers.Dense(256))\n",
    "model.add(attention())\n",
    "\n",
    "model.add(layers.Dense(2,activation=\"softmax\"))\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirection  (None, 101, 256)          183296    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 99, 64)            49216     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 49, 64)            0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  (None, 49, 256)           197632    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense (Dense)               (None, 49, 256)           65792     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 49, 256)           65792     \n",
      "                                                                 \n",
      " attention (attention)       (None, 256)               305       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 562547 (2.15 MB)\n",
      "Trainable params: 562547 (2.15 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1265/1265 [==============================] - 247s 189ms/step - loss: 0.5582 - accuracy: 0.7403 - val_loss: 0.5276 - val_accuracy: 0.7735\n",
      "Epoch 2/15\n",
      "1265/1265 [==============================] - 241s 190ms/step - loss: 0.4464 - accuracy: 0.7900 - val_loss: 0.3584 - val_accuracy: 0.8237\n",
      "Epoch 3/15\n",
      "1265/1265 [==============================] - 237s 187ms/step - loss: 0.2935 - accuracy: 0.8610 - val_loss: 0.2639 - val_accuracy: 0.8766\n",
      "Epoch 4/15\n",
      "1265/1265 [==============================] - 247s 195ms/step - loss: 0.2073 - accuracy: 0.9077 - val_loss: 0.2415 - val_accuracy: 0.8857\n",
      "Epoch 5/15\n",
      "1265/1265 [==============================] - 248s 196ms/step - loss: 0.1569 - accuracy: 0.9351 - val_loss: 0.1847 - val_accuracy: 0.9200\n",
      "Epoch 6/15\n",
      "1265/1265 [==============================] - 227s 180ms/step - loss: 0.1211 - accuracy: 0.9524 - val_loss: 0.1455 - val_accuracy: 0.9411\n",
      "Epoch 7/15\n",
      "1265/1265 [==============================] - 245s 194ms/step - loss: 0.0954 - accuracy: 0.9637 - val_loss: 0.1311 - val_accuracy: 0.9511\n",
      "Epoch 8/15\n",
      "1265/1265 [==============================] - 249s 197ms/step - loss: 0.0828 - accuracy: 0.9697 - val_loss: 0.1355 - val_accuracy: 0.9493\n",
      "Epoch 9/15\n",
      "1265/1265 [==============================] - 249s 197ms/step - loss: 0.0699 - accuracy: 0.9741 - val_loss: 0.1465 - val_accuracy: 0.9478\n",
      "Epoch 10/15\n",
      "1265/1265 [==============================] - 247s 195ms/step - loss: 0.0597 - accuracy: 0.9775 - val_loss: 0.1208 - val_accuracy: 0.9533\n",
      "Epoch 11/15\n",
      "1265/1265 [==============================] - 246s 195ms/step - loss: 0.0523 - accuracy: 0.9809 - val_loss: 0.1212 - val_accuracy: 0.9602\n",
      "Epoch 12/15\n",
      "1265/1265 [==============================] - 248s 196ms/step - loss: 0.0458 - accuracy: 0.9839 - val_loss: 0.1149 - val_accuracy: 0.9613\n",
      "Epoch 13/15\n",
      "1265/1265 [==============================] - 249s 197ms/step - loss: 0.0418 - accuracy: 0.9852 - val_loss: 0.1208 - val_accuracy: 0.9653\n",
      "Epoch 14/15\n",
      "1265/1265 [==============================] - 242s 191ms/step - loss: 0.0393 - accuracy: 0.9866 - val_loss: 0.1143 - val_accuracy: 0.9682\n",
      "Epoch 15/15\n",
      "1265/1265 [==============================] - 242s 191ms/step - loss: 0.0366 - accuracy: 0.9878 - val_loss: 0.1342 - val_accuracy: 0.9615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2877737af80>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(list(concatTrain)),np.array(list(yTrainBinary)),\n",
    "          validation_data=(np.array(list(concatVal)),np.array(list(yValBinary))),epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1265/1265 [==============================] - 232s 183ms/step - loss: 0.0363 - accuracy: 0.9874 - val_loss: 0.1134 - val_accuracy: 0.9613\n",
      "Epoch 2/3\n",
      "1265/1265 [==============================] - 237s 187ms/step - loss: 0.0307 - accuracy: 0.9895 - val_loss: 0.1486 - val_accuracy: 0.9602\n",
      "Epoch 3/3\n",
      "1265/1265 [==============================] - 239s 189ms/step - loss: 0.0279 - accuracy: 0.9907 - val_loss: 0.1211 - val_accuracy: 0.9662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2878062a3b0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(np.array(list(concatTrain)),np.array(list(yTrainBinary)),\n",
    "          validation_data=(np.array(list(concatVal)),np.array(list(yValBinary))),epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 12s 74ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(concatTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predsActual = np.array(list(map(lambda x: np.argmax(x),preds)))\n",
    "predsActual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.974389755902361"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(yTestBinary,predsActual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data for training the second order model\n",
    "def getSecondOrderData(x,y):\n",
    "    newX = []\n",
    "    newY = []\n",
    "    for i in range(len(y)):\n",
    "        if (y[i]!=3):\n",
    "            newX.append(x[i])\n",
    "            newY.append(y[i])\n",
    "    return (np.array(newX),np.array(newY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainTwo, yTrainTwo = getSecondOrderData(concatTrain,yTrain)\n",
    "xValTwo, yValTwo = getSecondOrderData(concatVal, yVal)\n",
    "xTestTwo, yTestTwo = getSecondOrderData(concatTest, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelTwo = tf.keras.Sequential()\n",
    "modelTwo.add(tf.keras.Input(shape=(101,50),dtype=tf.float32))\n",
    "\n",
    "modelTwo.add(layers.Bidirectional(layers.LSTM(128,return_sequences=True)))\n",
    "\n",
    "modelTwo.add(layers.Conv1D(64,3))\n",
    "modelTwo.add(layers.MaxPooling1D())\n",
    "\n",
    "modelTwo.add(layers.Bidirectional(layers.LSTM(128,return_sequences=True)))\n",
    "\n",
    "modelTwo.add(layers.Dense(256))\n",
    "modelTwo.add(layers.Dense(256))\n",
    "modelTwo.add(attention())\n",
    "\n",
    "modelTwo.add(layers.Dense(3,activation=\"softmax\"))\n",
    "modelTwo.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "342/342 [==============================] - 63s 184ms/step - loss: 0.0343 - accuracy: 0.9875 - val_loss: 0.1958 - val_accuracy: 0.9507\n",
      "Epoch 2/5\n",
      "342/342 [==============================] - 61s 177ms/step - loss: 0.0356 - accuracy: 0.9879 - val_loss: 0.2157 - val_accuracy: 0.9548\n",
      "Epoch 3/5\n",
      "342/342 [==============================] - 60s 177ms/step - loss: 0.0330 - accuracy: 0.9878 - val_loss: 0.2263 - val_accuracy: 0.9431\n",
      "Epoch 4/5\n",
      "342/342 [==============================] - 61s 178ms/step - loss: 0.0268 - accuracy: 0.9896 - val_loss: 0.2440 - val_accuracy: 0.9448\n",
      "Epoch 5/5\n",
      "342/342 [==============================] - 62s 181ms/step - loss: 0.0373 - accuracy: 0.9862 - val_loss: 0.1630 - val_accuracy: 0.9607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x287772cc340>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelTwo.fit(np.array(list(xTrainTwo)),np.array(list(yTrainTwo)),\n",
    "          validation_data=(np.array(list(xValTwo)),np.array(list(yValTwo))),epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "342/342 [==============================] - 68s 198ms/step - loss: 0.0303 - accuracy: 0.9895 - val_loss: 0.1889 - val_accuracy: 0.9540\n",
      "Epoch 2/2\n",
      "342/342 [==============================] - 67s 197ms/step - loss: 0.0288 - accuracy: 0.9905 - val_loss: 0.1947 - val_accuracy: 0.9582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x28780a0cdc0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelTwo.fit(np.array(list(xTrainTwo)),np.array(list(yTrainTwo)),\n",
    "          validation_data=(np.array(list(xValTwo)),np.array(list(yValTwo))),epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 3s 77ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2, ..., 2, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = modelTwo.predict(xTestTwo)\n",
    "predsActual = np.array(list(map(lambda x: np.argmax(x),preds)))\n",
    "predsActual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.968437259430331"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(yTestTwo,predsActual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 11s 68ms/step\n",
      "157/157 [==============================] - 10s 66ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "# Get First order predictions\n",
    "finalPreds = []\n",
    "orderOnePreds = np.array(list(map(lambda x: np.argmax(x),model.predict(concatTest))))\n",
    "orderTwoPreds = np.array(list(map(lambda x: np.argmax(x),modelTwo.predict(concatTest))))\n",
    "for i in range(len(orderOnePreds)):\n",
    "    if (orderOnePreds[i]==0):\n",
    "        finalPreds.append(3)\n",
    "    else:\n",
    "        finalPreds.append(orderTwoPreds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[354, 73, 902, 3669]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = [0,0,0,0]\n",
    "for i in finalPreds:\n",
    "    counts[i]+=1\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9667867146858744"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(finalPreds,yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model,open('Model1','wb'))\n",
    "pickle.dump(modelTwo,open('Model2','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
